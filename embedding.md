# Embedding

'맑음', '흐림'과 같은 특성을 범주형 변수(categorical variable)라고 하는데, 이를 0, 1과 같은 수치 데이터로 변환하여 처리하여야 합니다. 이렇게 수치로 변환한 데이터를 더미변수 (dummy variable)이라고 하고, sikit-learn의 LabelEncoding class나 OneHotEncoder class를 사용하여 범주형 변수를 더미 변수로 변환할 수 있습니다. 

또한, 자연어를 컴퓨터가 처리하도록 하기 위해서 문자를 숫자로 변환하는 과정이 필요합니다. 문자를 기계가 이해할 수 있는 숫자로 바꾼 결과 또는 그 과정을 임베딩(Embedding)이라고 합니다. 

- 고정된 크기의 실수 벡터로 변환합니다.
- 실수값의 크기가 의미를 가지므로 단어간의 관계 표현이 가능합니다.
- 실수값은 임의로 정하지 않고 데이터를 이용해 학습하여 구합니다. 

[Embedding을 이용하기](https://github.com/kyopark2014/ML-Algorithms/blob/main/rnn-simple.md#embedding%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%98%EA%B8%B0)에서는 500개의 단어를 16개의 실수로 이루어진 벡터로 변환합니다. 

## One-Hot Encoding

가장 간단한 형태의 임베딩은 문장에 어떤 단어가 많이 쓰였는지를 파악하여 글쓴이의 의도를 알 수 있습니다. 원-핫 인코딩(One-Hot Encoding)은 여러 기법 중 단어를 표현하는 가장 기본적인 표현 방법입니다.

원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 벡터 표현 방식입니다. 이렇게 표현된 벡터를 원-핫 벡터(One-Hot Vector)라고 합니다. 텍스트를 숫자로 표현하는 대표적인 방법으로 하나의 1과 수많은 0으로 표현하는 방법입니다. 

원-핫 인코딩을 위해서 먼저 해야할 일은 단어 집합을 만드는 일입니다. 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓으면 이를 단어 집합이라고 합니다. 그리고 이 단어 집합에 고유한 정수를 부여하는 정수 인코딩을 진행합니다. 텍스트에 단어가 총 5,000개가 존재한다면, 단어 집합의 크기는 5,000입니다.

원-핫 인코딩은 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있습니다. 다른 표현으로는 벡터의 차원이 늘어난다고 표현합니다. 원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 됩니다. 가령, 단어가 1,000개인 코퍼스를 가지고 원 핫 벡터를 만들면, 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 됩니다. 이와같이 어휘 사전의 Token수 만큼 배열의 크기를 정해야해서 데이터가 커지며. 의미 유사성, Collocation(같이 쓰임) 등을 포함 할 수 없습니다. 

단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지가 있습니다. 첫째는 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있으며, 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있습니다.



## Word Embedding

단어의 의미를 최대한 담아 벡터로 바꿉니다. Word embedding에서는 One-Hot encoding과 달리 하나의 단어가 미리 정의된 차원에서 연속형의 값을 갖는 벡터로 표현됩니다. 

Embedding의 하나의 방법으로 [Word2Vec](https://word2vec.kr/search/)가 있는데, "한국-서울+도쿄"로 입력하면 아래와 같은 결과를 얻을 수 있습니다. 

![image](https://user-images.githubusercontent.com/52392004/188033702-4562c422-0653-466c-8929-7a6a684d3252.png)


## Reference 

[Machine Learning at Work - 한빛미디어]

[AI4SCHOOL 언어와 소통 – 5. 자연어처리 기법](http://ai4school.org/?page_id=2407)

[딥 러닝을 이용한 자연어 처리 입문 - 원-핫 인코딩(One-Hot Encoding)](https://wikidocs.net/22647)
