{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa0cbb7",
   "metadata": {},
   "source": [
    "# 8.4 안전 운전자 예측 경진대회 성능 개선 I : LightGBM 모델\n",
    "\n",
    "[[ch8] LGB Modeling](https://www.kaggle.com/code/werooring/ch8-lgb-modeling/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae65f84",
   "metadata": {},
   "source": [
    "[안전 운전자 예측 경진대회 링크](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction)\n",
    "\n",
    "[탐색적 데이터 분석 코드 참고 링크](https://www.kaggle.com/code/bertcarremans/data-preparation-exploration/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f824826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 경로\n",
    "data_path = './porto-seguro-safe-driver-prediction/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv', index_col='id')\n",
    "test = pd.read_csv(data_path + 'test.csv', index_col='id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612671ad",
   "metadata": {},
   "source": [
    "### 8.4.1 피처 엔지니어링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e659c",
   "metadata": {},
   "source": [
    "### 데이터 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a11099",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis=1) # 타깃값 제거\n",
    "\n",
    "all_features = all_data.columns # 전체 피처"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b2a34e",
   "metadata": {},
   "source": [
    "### 명목형 피처 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2addfc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 명목형 피처\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature] \n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3cb93f",
   "metadata": {},
   "source": [
    "### 파생 피처 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b249b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '데이터 하나당 결측값 개수'를 파생 피처로 추가\n",
    "all_data['num_missing'] = (all_data==-1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd52819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명목형 피처, calc 분류의 피처를 제외한 피처\n",
    "remaining_features = [feature for feature in all_features\n",
    "                      if ('cat' not in feature and 'calc' not in feature)] \n",
    "# num_missing을 remaining_features에 추가\n",
    "remaining_features.append('num_missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96eaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류가 ind인 피처\n",
    "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
    "\n",
    "is_first_feature = True\n",
    "for ind_feature in ind_features:\n",
    "    if is_first_feature:\n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '_'\n",
    "        is_first_feature = False\n",
    "    else:\n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aede944b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2_2_5_1_0_0_1_0_0_0_0_0_0_0_11_0_1_0_\n",
       "1           1_1_7_0_0_0_0_1_0_0_0_0_0_0_3_0_0_1_\n",
       "2          5_4_9_1_0_0_0_1_0_0_0_0_0_0_12_1_0_0_\n",
       "3           0_1_2_0_0_1_0_0_0_0_0_0_0_0_8_1_0_0_\n",
       "4           0_2_0_1_0_1_0_0_0_0_0_0_0_0_9_1_0_0_\n",
       "                           ...                  \n",
       "1488023     0_1_6_0_0_0_1_0_0_0_0_0_0_0_2_0_0_1_\n",
       "1488024    5_3_5_1_0_0_0_1_0_0_0_0_0_0_11_1_0_0_\n",
       "1488025     0_1_5_0_0_1_0_0_0_0_0_0_0_0_5_0_0_1_\n",
       "1488026    6_1_5_1_0_0_0_0_1_0_0_0_0_0_13_1_0_0_\n",
       "1488027    7_1_4_1_0_0_0_0_1_0_0_0_0_0_12_1_0_0_\n",
       "Name: mix_ind, Length: 1488028, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['mix_ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95575ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_count_features = []\n",
    "for feature in cat_features+['mix_ind']:\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: \n",
    "                                                           val_counts_dict[x])\n",
    "    cat_count_features.append(f'{feature}_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f0a973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat_count',\n",
       " 'ps_ind_04_cat_count',\n",
       " 'ps_ind_05_cat_count',\n",
       " 'ps_car_01_cat_count',\n",
       " 'ps_car_02_cat_count',\n",
       " 'ps_car_03_cat_count',\n",
       " 'ps_car_04_cat_count',\n",
       " 'ps_car_05_cat_count',\n",
       " 'ps_car_06_cat_count',\n",
       " 'ps_car_07_cat_count',\n",
       " 'ps_car_08_cat_count',\n",
       " 'ps_car_09_cat_count',\n",
       " 'ps_car_10_cat_count',\n",
       " 'ps_car_11_cat_count',\n",
       " 'mix_ind_count']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_count_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2637263e",
   "metadata": {},
   "source": [
    "### 필요 없는 피처 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3508aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "# 필요 없는 피처들\n",
    "drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin', \n",
    "                 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n",
    "\n",
    "# remaining_features, cat_count_features에서 drop_features를 제거한 데이터\n",
    "all_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n",
    "\n",
    "# 데이터 합치기\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n",
    "                               encoded_cat_matrix],\n",
    "                              format='csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8bce8",
   "metadata": {},
   "source": [
    "### 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "889e2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8a2fe",
   "metadata": {},
   "source": [
    "### 정규화 지니계수 계산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e5f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true, y_pred):\n",
    "    # 실제값과 예측값의 크기가 같은지 확인 (값이 다르면 오류 발생)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    n_samples = y_true.shape[0]                      # 데이터 개수\n",
    "    L_mid = np.linspace(1 / n_samples, 1, n_samples) # 대각선 값\n",
    "\n",
    "    # 1) 예측값에 대한 지니계수\n",
    "    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "    G_pred = np.sum(L_mid - L_pred)       # 예측 값에 대한 지니계수\n",
    "\n",
    "    # 2) 예측이 완벽할 때 지니계수\n",
    "    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    G_true = np.sum(L_mid - L_true)       # 예측이 완벽할 때 지니계수\n",
    "\n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3598805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM용 gini() 함수\n",
    "def gini(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', eval_gini(labels, preds), True # 반환값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946bb7ea",
   "metadata": {},
   "source": [
    "### 8.4.2 하이퍼파라미터 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b53f72",
   "metadata": {},
   "source": [
    "### 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2402912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=0.2, \n",
    "                                                      random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "bayes_dtrain = lgb.Dataset(X_train, y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628e5557",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 범위 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0781d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'num_leaves': (30, 40),\n",
    "                'lambda_l1': (0.7, 0.9),\n",
    "                'lambda_l2': (0.9, 1),\n",
    "                'feature_fraction': (0.6, 0.7),\n",
    "                'bagging_fraction': (0.6, 0.9),\n",
    "                'min_child_samples': (6, 10),\n",
    "                'min_child_weight': (10, 40)}\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "fixed_params = {'objective': 'binary',\n",
    "                'learning_rate': 0.005,\n",
    "                'bagging_freq': 1,\n",
    "                'force_row_wise': True,\n",
    "                'random_state': 1991}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f45142",
   "metadata": {},
   "source": [
    "### (베이지안 최적화용) 평가지표 계산 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8864e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n",
    "                  bagging_fraction, min_child_samples, min_child_weight):\n",
    "    '''최적화하려는 평가지표(지니계수) 계산 함수'''\n",
    "    \n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터 \n",
    "    params = {'num_leaves': int(round(num_leaves)),\n",
    "              'lambda_l1': lambda_l1,\n",
    "              'lambda_l2': lambda_l2,\n",
    "              'feature_fraction': feature_fraction,\n",
    "              'bagging_fraction': bagging_fraction,\n",
    "              'min_child_samples': int(round(min_child_samples)),\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'feature_pre_filter': False}\n",
    "    # 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "    \n",
    "    print('하이퍼파라미터:', params)    \n",
    "    \n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params=params, \n",
    "                           train_set=bayes_dtrain,\n",
    "                           num_boost_round=2500,\n",
    "                           valid_sets=bayes_dvalid,\n",
    "                           feval=gini,\n",
    "                           early_stopping_rounds=300,\n",
    "                           verbose_eval=False)\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = lgb_model.predict(X_valid) \n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "    \n",
    "    return gini_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3c517",
   "metadata": {},
   "source": [
    "### 최적화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6b5dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: bayesian-optimization in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from bayesian-optimization) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from bayesian-optimization) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from bayesian-optimization) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b24cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer = BayesianOptimization(f=eval_function,      # 평가지표 계산 함수\n",
    "                                 pbounds=param_bounds, # 하이퍼파라미터 범위\n",
    "                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc39913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2855811556220905\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.2856  \u001b[0m | \u001b[0m 0.7646  \u001b[0m | \u001b[0m 0.6715  \u001b[0m | \u001b[0m 0.8206  \u001b[0m | \u001b[0m 0.9545  \u001b[0m | \u001b[0m 7.695   \u001b[0m | \u001b[0m 29.38   \u001b[0m | \u001b[0m 34.38   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2837380537005777\n",
      "\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.2837  \u001b[0m | \u001b[0m 0.8675  \u001b[0m | \u001b[0m 0.6964  \u001b[0m | \u001b[0m 0.7767  \u001b[0m | \u001b[0m 0.9792  \u001b[0m | \u001b[0m 8.116   \u001b[0m | \u001b[0m 27.04   \u001b[0m | \u001b[0m 39.26   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2857848354322048\n",
      "\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.2858  \u001b[0m | \u001b[95m 0.6213  \u001b[0m | \u001b[95m 0.6087  \u001b[0m | \u001b[95m 0.704   \u001b[0m | \u001b[95m 0.9833  \u001b[0m | \u001b[95m 9.113   \u001b[0m | \u001b[95m 36.1    \u001b[0m | \u001b[95m 39.79   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.8444997594874222, 'lambda_l2': 0.9234023852202012, 'feature_fraction': 0.6593983245038058, 'bagging_fraction': 0.8977977822397395, 'min_child_samples': 9, 'min_child_weight': 10.549362495448534, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2828993761731121\n",
      "\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.2829  \u001b[0m | \u001b[0m 0.8978  \u001b[0m | \u001b[0m 0.6594  \u001b[0m | \u001b[0m 0.8445  \u001b[0m | \u001b[0m 0.9234  \u001b[0m | \u001b[0m 8.619   \u001b[0m | \u001b[0m 10.55   \u001b[0m | \u001b[0m 30.09   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.7738449330497988, 'lambda_l2': 0.9032695189818599, 'feature_fraction': 0.6606341064409726, 'bagging_fraction': 0.7666713964943057, 'min_child_samples': 9, 'min_child_weight': 29.306172421380474, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28513273331754563\n",
      "\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.2851  \u001b[0m | \u001b[0m 0.7667  \u001b[0m | \u001b[0m 0.6606  \u001b[0m | \u001b[0m 0.7738  \u001b[0m | \u001b[0m 0.9033  \u001b[0m | \u001b[0m 8.769   \u001b[0m | \u001b[0m 29.31   \u001b[0m | \u001b[0m 36.6    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 33, 'lambda_l1': 0.8248456547917442, 'lambda_l2': 0.9, 'feature_fraction': 0.6945450446349253, 'bagging_fraction': 0.6591082581129564, 'min_child_samples': 10, 'min_child_weight': 35.85860401520264, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2851356163991722\n",
      "\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.2851  \u001b[0m | \u001b[0m 0.6591  \u001b[0m | \u001b[0m 0.6945  \u001b[0m | \u001b[0m 0.8248  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 9.818   \u001b[0m | \u001b[0m 35.86   \u001b[0m | \u001b[0m 32.79   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.8720599498548245, 'lambda_l2': 0.9769965142032127, 'feature_fraction': 0.6770410324297845, 'bagging_fraction': 0.6283750235094676, 'min_child_samples': 6, 'min_child_weight': 39.88993616984878, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2863207772581034\n",
      "\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.2863  \u001b[0m | \u001b[95m 0.6284  \u001b[0m | \u001b[95m 0.677   \u001b[0m | \u001b[95m 0.8721  \u001b[0m | \u001b[95m 0.977   \u001b[0m | \u001b[95m 6.163   \u001b[0m | \u001b[95m 39.89   \u001b[0m | \u001b[95m 39.49   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 35, 'lambda_l1': 0.7881579420320822, 'lambda_l2': 0.9341538128137851, 'feature_fraction': 0.6337001812462025, 'bagging_fraction': 0.7425268652158652, 'min_child_samples': 6, 'min_child_weight': 39.820494958060664, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28668675475018845\n",
      "\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.2867  \u001b[0m | \u001b[95m 0.7425  \u001b[0m | \u001b[95m 0.6337  \u001b[0m | \u001b[95m 0.7882  \u001b[0m | \u001b[95m 0.9342  \u001b[0m | \u001b[95m 6.036   \u001b[0m | \u001b[95m 39.82   \u001b[0m | \u001b[95m 35.13   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.7942298723143388, 'lambda_l2': 0.9341906136292961, 'feature_fraction': 0.6014736366292884, 'bagging_fraction': 0.8901672711052453, 'min_child_samples': 6, 'min_child_weight': 39.73902941168508, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28486372926665016\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.2849  \u001b[0m | \u001b[0m 0.8902  \u001b[0m | \u001b[0m 0.6015  \u001b[0m | \u001b[0m 0.7942  \u001b[0m | \u001b[0m 0.9342  \u001b[0m | \u001b[0m 6.199   \u001b[0m | \u001b[0m 39.74   \u001b[0m | \u001b[0m 30.33   \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화 수행\n",
    "optimizer.maximize(init_points=3, n_iter=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1924d297",
   "metadata": {},
   "source": [
    "### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41d1c3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.7425268652158652,\n",
       " 'feature_fraction': 0.6337001812462025,\n",
       " 'lambda_l1': 0.7881579420320822,\n",
       " 'lambda_l2': 0.9341538128137851,\n",
       " 'min_child_samples': 6.03624517464404,\n",
       " 'min_child_weight': 39.820494958060664,\n",
       " 'num_leaves': 35.128706389767935}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가함수 점수가 최대일 때 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9d82ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7386301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "max_params.update(fixed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d919ffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.7425268652158652,\n",
       " 'feature_fraction': 0.6337001812462025,\n",
       " 'lambda_l1': 0.7881579420320822,\n",
       " 'lambda_l2': 0.9341538128137851,\n",
       " 'min_child_samples': 6,\n",
       " 'min_child_weight': 39.820494958060664,\n",
       " 'num_leaves': 35,\n",
       " 'objective': 'binary',\n",
       " 'learning_rate': 0.005,\n",
       " 'bagging_freq': 1,\n",
       " 'force_row_wise': True,\n",
       " 'random_state': 1991}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6813ae2",
   "metadata": {},
   "source": [
    "### 8.4.3 모델 훈련 및 성능 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a0974fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1556\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154275\tvalid_0's gini: 0.268414\n",
      "[200]\tvalid_0's binary_logloss: 0.153216\tvalid_0's gini: 0.274179\n",
      "[300]\tvalid_0's binary_logloss: 0.152613\tvalid_0's gini: 0.279279\n",
      "[400]\tvalid_0's binary_logloss: 0.152254\tvalid_0's gini: 0.28263\n",
      "[500]\tvalid_0's binary_logloss: 0.152018\tvalid_0's gini: 0.285558\n",
      "[600]\tvalid_0's binary_logloss: 0.151855\tvalid_0's gini: 0.288154\n",
      "[700]\tvalid_0's binary_logloss: 0.151735\tvalid_0's gini: 0.290386\n",
      "[800]\tvalid_0's binary_logloss: 0.151644\tvalid_0's gini: 0.292188\n",
      "[900]\tvalid_0's binary_logloss: 0.15158\tvalid_0's gini: 0.293569\n",
      "[1000]\tvalid_0's binary_logloss: 0.151531\tvalid_0's gini: 0.29451\n",
      "[1100]\tvalid_0's binary_logloss: 0.151493\tvalid_0's gini: 0.295288\n",
      "[1200]\tvalid_0's binary_logloss: 0.151464\tvalid_0's gini: 0.295942\n",
      "[1300]\tvalid_0's binary_logloss: 0.151438\tvalid_0's gini: 0.296578\n",
      "[1400]\tvalid_0's binary_logloss: 0.151423\tvalid_0's gini: 0.296961\n",
      "[1500]\tvalid_0's binary_logloss: 0.151403\tvalid_0's gini: 0.297518\n",
      "[1600]\tvalid_0's binary_logloss: 0.151384\tvalid_0's gini: 0.297983\n",
      "[1700]\tvalid_0's binary_logloss: 0.151373\tvalid_0's gini: 0.298225\n",
      "[1800]\tvalid_0's binary_logloss: 0.151366\tvalid_0's gini: 0.298424\n",
      "[1900]\tvalid_0's binary_logloss: 0.15136\tvalid_0's gini: 0.298552\n",
      "[2000]\tvalid_0's binary_logloss: 0.151355\tvalid_0's gini: 0.298604\n",
      "[2100]\tvalid_0's binary_logloss: 0.151353\tvalid_0's gini: 0.29867\n",
      "[2200]\tvalid_0's binary_logloss: 0.151352\tvalid_0's gini: 0.298709\n",
      "[2300]\tvalid_0's binary_logloss: 0.151355\tvalid_0's gini: 0.298629\n",
      "Early stopping, best iteration is:\n",
      "[2031]\tvalid_0's binary_logloss: 0.151351\tvalid_0's gini: 0.298756\n",
      "폴드 1 지니계수 : 0.2987558891222359\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1560\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154379\tvalid_0's gini: 0.256725\n",
      "[200]\tvalid_0's binary_logloss: 0.153388\tvalid_0's gini: 0.262264\n",
      "[300]\tvalid_0's binary_logloss: 0.152843\tvalid_0's gini: 0.266938\n",
      "[400]\tvalid_0's binary_logloss: 0.152532\tvalid_0's gini: 0.269836\n",
      "[500]\tvalid_0's binary_logloss: 0.152323\tvalid_0's gini: 0.272901\n",
      "[600]\tvalid_0's binary_logloss: 0.152185\tvalid_0's gini: 0.27516\n",
      "[700]\tvalid_0's binary_logloss: 0.15209\tvalid_0's gini: 0.276771\n",
      "[800]\tvalid_0's binary_logloss: 0.152017\tvalid_0's gini: 0.278423\n",
      "[900]\tvalid_0's binary_logloss: 0.151957\tvalid_0's gini: 0.27977\n",
      "[1000]\tvalid_0's binary_logloss: 0.151911\tvalid_0's gini: 0.281039\n",
      "[1100]\tvalid_0's binary_logloss: 0.151874\tvalid_0's gini: 0.281925\n",
      "[1200]\tvalid_0's binary_logloss: 0.151843\tvalid_0's gini: 0.282706\n",
      "[1300]\tvalid_0's binary_logloss: 0.151812\tvalid_0's gini: 0.283602\n",
      "[1400]\tvalid_0's binary_logloss: 0.151798\tvalid_0's gini: 0.284018\n",
      "[1500]\tvalid_0's binary_logloss: 0.151779\tvalid_0's gini: 0.284513\n",
      "[1600]\tvalid_0's binary_logloss: 0.151771\tvalid_0's gini: 0.284723\n",
      "[1700]\tvalid_0's binary_logloss: 0.151763\tvalid_0's gini: 0.284887\n",
      "[1800]\tvalid_0's binary_logloss: 0.151757\tvalid_0's gini: 0.285084\n",
      "[1900]\tvalid_0's binary_logloss: 0.151751\tvalid_0's gini: 0.285319\n",
      "[2000]\tvalid_0's binary_logloss: 0.15175\tvalid_0's gini: 0.285417\n",
      "[2100]\tvalid_0's binary_logloss: 0.151744\tvalid_0's gini: 0.285595\n",
      "[2200]\tvalid_0's binary_logloss: 0.151743\tvalid_0's gini: 0.285673\n",
      "[2300]\tvalid_0's binary_logloss: 0.151739\tvalid_0's gini: 0.28579\n",
      "[2400]\tvalid_0's binary_logloss: 0.151733\tvalid_0's gini: 0.285964\n",
      "[2500]\tvalid_0's binary_logloss: 0.151726\tvalid_0's gini: 0.286204\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.151726\tvalid_0's gini: 0.286204\n",
      "폴드 2 지니계수 : 0.2862043002812519\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154284\tvalid_0's gini: 0.260449\n",
      "[200]\tvalid_0's binary_logloss: 0.153222\tvalid_0's gini: 0.266628\n",
      "[300]\tvalid_0's binary_logloss: 0.152631\tvalid_0's gini: 0.271052\n",
      "[400]\tvalid_0's binary_logloss: 0.152273\tvalid_0's gini: 0.274558\n",
      "[500]\tvalid_0's binary_logloss: 0.152044\tvalid_0's gini: 0.277131\n",
      "[600]\tvalid_0's binary_logloss: 0.15189\tvalid_0's gini: 0.279298\n",
      "[700]\tvalid_0's binary_logloss: 0.151787\tvalid_0's gini: 0.280873\n",
      "[800]\tvalid_0's binary_logloss: 0.15172\tvalid_0's gini: 0.282003\n",
      "[900]\tvalid_0's binary_logloss: 0.151663\tvalid_0's gini: 0.283155\n",
      "[1000]\tvalid_0's binary_logloss: 0.151627\tvalid_0's gini: 0.283871\n",
      "[1100]\tvalid_0's binary_logloss: 0.151605\tvalid_0's gini: 0.284206\n",
      "[1200]\tvalid_0's binary_logloss: 0.151586\tvalid_0's gini: 0.284579\n",
      "[1300]\tvalid_0's binary_logloss: 0.151574\tvalid_0's gini: 0.284972\n",
      "[1400]\tvalid_0's binary_logloss: 0.151568\tvalid_0's gini: 0.285119\n",
      "[1500]\tvalid_0's binary_logloss: 0.151566\tvalid_0's gini: 0.285189\n",
      "[1600]\tvalid_0's binary_logloss: 0.151561\tvalid_0's gini: 0.285213\n",
      "[1700]\tvalid_0's binary_logloss: 0.151561\tvalid_0's gini: 0.285186\n",
      "[1800]\tvalid_0's binary_logloss: 0.151565\tvalid_0's gini: 0.285135\n",
      "[1900]\tvalid_0's binary_logloss: 0.151567\tvalid_0's gini: 0.285032\n",
      "Early stopping, best iteration is:\n",
      "[1643]\tvalid_0's binary_logloss: 0.151556\tvalid_0's gini: 0.285286\n",
      "폴드 3 지니계수 : 0.2852864624281034\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1557\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154363\tvalid_0's gini: 0.254746\n",
      "[200]\tvalid_0's binary_logloss: 0.153348\tvalid_0's gini: 0.260519\n",
      "[300]\tvalid_0's binary_logloss: 0.152798\tvalid_0's gini: 0.26467\n",
      "[400]\tvalid_0's binary_logloss: 0.152471\tvalid_0's gini: 0.26803\n",
      "[500]\tvalid_0's binary_logloss: 0.152266\tvalid_0's gini: 0.270787\n",
      "[600]\tvalid_0's binary_logloss: 0.152133\tvalid_0's gini: 0.272804\n",
      "[700]\tvalid_0's binary_logloss: 0.152042\tvalid_0's gini: 0.274335\n",
      "[800]\tvalid_0's binary_logloss: 0.151973\tvalid_0's gini: 0.275739\n",
      "[900]\tvalid_0's binary_logloss: 0.151925\tvalid_0's gini: 0.276678\n",
      "[1000]\tvalid_0's binary_logloss: 0.151887\tvalid_0's gini: 0.277539\n",
      "[1100]\tvalid_0's binary_logloss: 0.151867\tvalid_0's gini: 0.277906\n",
      "[1200]\tvalid_0's binary_logloss: 0.151852\tvalid_0's gini: 0.278265\n",
      "[1300]\tvalid_0's binary_logloss: 0.151837\tvalid_0's gini: 0.278706\n",
      "[1400]\tvalid_0's binary_logloss: 0.151828\tvalid_0's gini: 0.278944\n",
      "[1500]\tvalid_0's binary_logloss: 0.151822\tvalid_0's gini: 0.279167\n",
      "[1600]\tvalid_0's binary_logloss: 0.151822\tvalid_0's gini: 0.27919\n",
      "[1700]\tvalid_0's binary_logloss: 0.151817\tvalid_0's gini: 0.279426\n",
      "[1800]\tvalid_0's binary_logloss: 0.151818\tvalid_0's gini: 0.279468\n",
      "[1900]\tvalid_0's binary_logloss: 0.151819\tvalid_0's gini: 0.279538\n",
      "Early stopping, best iteration is:\n",
      "[1674]\tvalid_0's binary_logloss: 0.151815\tvalid_0's gini: 0.279446\n",
      "폴드 4 지니계수 : 0.2794460723324984\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154427\tvalid_0's gini: 0.263095\n",
      "[200]\tvalid_0's binary_logloss: 0.153428\tvalid_0's gini: 0.269457\n",
      "[300]\tvalid_0's binary_logloss: 0.15288\tvalid_0's gini: 0.273613\n",
      "[400]\tvalid_0's binary_logloss: 0.152551\tvalid_0's gini: 0.277158\n",
      "[500]\tvalid_0's binary_logloss: 0.15233\tvalid_0's gini: 0.280534\n",
      "[600]\tvalid_0's binary_logloss: 0.152173\tvalid_0's gini: 0.283537\n",
      "[700]\tvalid_0's binary_logloss: 0.152051\tvalid_0's gini: 0.286257\n",
      "[800]\tvalid_0's binary_logloss: 0.151957\tvalid_0's gini: 0.288418\n",
      "[900]\tvalid_0's binary_logloss: 0.151893\tvalid_0's gini: 0.290057\n",
      "[1000]\tvalid_0's binary_logloss: 0.151849\tvalid_0's gini: 0.291266\n",
      "[1100]\tvalid_0's binary_logloss: 0.151806\tvalid_0's gini: 0.292433\n",
      "[1200]\tvalid_0's binary_logloss: 0.15177\tvalid_0's gini: 0.293465\n",
      "[1300]\tvalid_0's binary_logloss: 0.151754\tvalid_0's gini: 0.293927\n",
      "[1400]\tvalid_0's binary_logloss: 0.151739\tvalid_0's gini: 0.294366\n",
      "[1500]\tvalid_0's binary_logloss: 0.151722\tvalid_0's gini: 0.294835\n",
      "[1600]\tvalid_0's binary_logloss: 0.151711\tvalid_0's gini: 0.295208\n",
      "[1700]\tvalid_0's binary_logloss: 0.1517\tvalid_0's gini: 0.295476\n",
      "[1800]\tvalid_0's binary_logloss: 0.151693\tvalid_0's gini: 0.295694\n",
      "[1900]\tvalid_0's binary_logloss: 0.151688\tvalid_0's gini: 0.295862\n",
      "[2000]\tvalid_0's binary_logloss: 0.151687\tvalid_0's gini: 0.295947\n",
      "[2100]\tvalid_0's binary_logloss: 0.151686\tvalid_0's gini: 0.296049\n",
      "[2200]\tvalid_0's binary_logloss: 0.151684\tvalid_0's gini: 0.296065\n",
      "[2300]\tvalid_0's binary_logloss: 0.15168\tvalid_0's gini: 0.296174\n",
      "[2400]\tvalid_0's binary_logloss: 0.151681\tvalid_0's gini: 0.296211\n",
      "[2500]\tvalid_0's binary_logloss: 0.151682\tvalid_0's gini: 0.296251\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2256]\tvalid_0's binary_logloss: 0.151678\tvalid_0's gini: 0.296173\n",
      "폴드 5 지니계수 : 0.29617346153678703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0]) \n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0]) \n",
    "\n",
    "# OOF 방식으로 모델 훈련, 검증, 예측\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train, y_train = X[train_idx], y[train_idx] # 훈련용 데이터\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx] # 검증용 데이터\n",
    "\n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train, y_train) # LightGBM 전용 훈련 데이터셋\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid) # LightGBM 전용 검증 데이터셋\n",
    "                          \n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params=max_params,    # 최적 하이퍼파라미터\n",
    "                          train_set=dtrain,     # 훈련 데이터셋\n",
    "                          num_boost_round=2500, # 부스팅 반복 횟수\n",
    "                          valid_sets=dvalid,    # 성능 평가용 검증 데이터셋\n",
    "                          feval=gini,           # 검증용 평가지표\n",
    "                          early_stopping_rounds=300, # 조기종료 조건\n",
    "                          verbose_eval=100)     # 100번째마다 점수 출력\n",
    "    \n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측 \n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "    \n",
    "    # 검증 데이터 예측확률에 대한 정규화 지니계수\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1} 지니계수 : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7296947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 검증 데이터 지니계수 : 0.2890999692058917\n"
     ]
    }
   ],
   "source": [
    "print('OOF 검증 데이터 지니계수 :', eval_gini(y, oof_val_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222533e",
   "metadata": {},
   "source": [
    "### 8.4.4 예측 및 결과 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ac1df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
